{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.r2_oos     = object()\n",
    "        self.r2_is      = object()\n",
    "        self.predictors = object()\n",
    "        self.summary    = object()\n",
    "        self.model      = object()\n",
    "        self.fitted_model = object()\n",
    "        self.X_train    = object()\n",
    "        self.y_train    = object()\n",
    "        self.X_test     = object()\n",
    "        self.y_test     = object()\n",
    "        self.pred       = object()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load(r'../data/master_prepared.npy', allow_pickle=True))\n",
    "cols = np.load(r'../data/columns_prepared.npy', allow_pickle=True)\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Value\n",
    "df['mv_^0.5'] = df['mv'] ** 0.5\n",
    "\n",
    "# Book to Market Value\n",
    "df['bvtmv_^2'] = df['bvtmv'] ** 2\n",
    "df['bvtmv_^3'] = df['bvtmv'] ** 3\n",
    "\n",
    "# Operating Margin\n",
    "df['opmarg_^2'] = df['opmarg'] ** 2\n",
    "df['opmarg_^3'] = df['opmarg'] ** 3\n",
    "\n",
    "# Free cash flow\n",
    "df['fcf_^2'] = df['fcf'] ** 2\n",
    "df['fcf_^3'] = df['fcf'] ** 3\n",
    "\n",
    "# Return on invested capital\n",
    "df['roic_^2'] = df['roic'] ** 2\n",
    "df['roic_^3'] = df['roic'] ** 3\n",
    "\n",
    "# Volatility\n",
    "df['std_12m_^2'] = df['std_12m'] ** 2\n",
    "df['std_12m_^3'] = df['std_12m'] ** 3\n",
    "\n",
    "# Return\n",
    "df['retxs_12m_^2'] = df['retxs_12m'] ** 2\n",
    "df['retxs_12m_^3'] = df['retxs_12m'] ** 3\n",
    "df['retxs_3m_^2'] = df['retxs_3m'] ** 2\n",
    "df['retxs_3m_^3'] = df['retxs_3m'] ** 3\n",
    "\n",
    "# Dividend yield\n",
    "df['dy_^2'] = df['dy'] ** 2\n",
    "df['dy_^3'] = df['dy'] ** 3\n",
    "\n",
    "# Beta\n",
    "df['beta_^2'] = df['beta'] ** 2\n",
    "df['beta_^3'] = df['beta'] ** 3\n",
    "\n",
    "# Shares Outstanding\n",
    "df['so_^2'] = df['so'] ** 2\n",
    "df['so_^3'] = df['so'] ** 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mv_bvtmv'] = df['bvtmv'] / df['mv']\n",
    "\n",
    "df['ret12m_bvtmv'] = df['ret_12m'] * df['mv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'date',\n",
    "    'eligible',\n",
    "    'companyid', \n",
    "    'ret_1f1',\n",
    "    'ret_1f3',\n",
    "    'ret_1f6',\n",
    "    'ret_1f12',\n",
    "    'retxs_1f1',\n",
    "    'retxs_1f3',\n",
    "    'retxs_1f6',\n",
    "    'retxs_1f12'\n",
    "]\n",
    "\n",
    "y_col = 'retxs_1f1'\n",
    "\n",
    "# X cols are the independent variables\n",
    "x_cols = [i for i in list(df.columns) if i not in drop_cols]\n",
    "\n",
    "# Only use eligible data\n",
    "df_eligible = df[df['eligible']==1]\n",
    "X = df_eligible[x_cols].astype('float64')\n",
    "y = df_eligible[y_col].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.dropna(how='any', inplace=True)\n",
    "y = y.loc[X.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shapes\n",
      "============\n",
      "X_train:  (25712, 80)\n",
      "y_train:  (25712,)\n",
      "X_test:   (668, 80)\n",
      "y_test:   (668,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(start_index, train_range):\n",
    "\n",
    "    # Dates\n",
    "    global dates\n",
    "    dates = df['date'].unique()\n",
    "    train_dates = dates[start_index:start_index+train_range+1]\n",
    "    test_dates = dates[start_index+train_range+1]\n",
    "\n",
    "    # Training Data\n",
    "    X_train = X[df_eligible['date'].isin(train_dates)].astype('float64')\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_train = y[df_eligible['date'].isin(train_dates)].astype('float64')\n",
    "\n",
    "    # Test Data\n",
    "    X_test = X[df_eligible['date']==test_dates].astype('float64')\n",
    "    X_test.insert(0, 'const', 1)\n",
    "    y_test = y[df_eligible['date']==test_dates].astype('float64')\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = prepare_data(60, 36)\n",
    "\n",
    "print(\"Input Shapes\")\n",
    "print(\"============\")\n",
    "print('X_train: ', X_train.values.shape)\n",
    "print('y_train: ', y_train.values.shape)\n",
    "print('X_test:  ', X_test.values.shape)\n",
    "print('y_test:  ', y_test.values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear        -0.01653885571103586\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:              retxs_1f1   R-squared (uncentered):                   0.105\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.105\n",
      "Method:                 Least Squares   F-statistic:                              252.3\n",
      "Date:                Thu, 13 Feb 2020   Prob (F-statistic):                        0.00\n",
      "Time:                        10:51:22   Log-Likelihood:                          14175.\n",
      "No. Observations:               25712   AIC:                                 -2.833e+04\n",
      "Df Residuals:                   25700   BIC:                                 -2.823e+04\n",
      "Df Model:                          12                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "mv         -4.721e-07   1.05e-07     -4.507      0.000   -6.77e-07   -2.67e-07\n",
      "bvtmv          0.0076      0.001      8.760      0.000       0.006       0.009\n",
      "ret_12m        0.0945      0.008     11.871      0.000       0.079       0.110\n",
      "retxs_12m     -0.0735      0.005    -13.597      0.000      -0.084      -0.063\n",
      "ret_6m         0.0077      0.004      2.022      0.043       0.000       0.015\n",
      "std_12m       -0.0239      0.004     -6.747      0.000      -0.031      -0.017\n",
      "fcf            0.0001   8.08e-05      1.675      0.094    -2.3e-05       0.000\n",
      "roic           0.0003   3.65e-05      7.766      0.000       0.000       0.000\n",
      "dy             0.0078      0.000     46.664      0.000       0.007       0.008\n",
      "beta           0.0045      0.001      3.133      0.002       0.002       0.007\n",
      "mom_3m12m      0.0152      0.004      3.368      0.001       0.006       0.024\n",
      "mom_12m36m     0.0054      0.001      5.632      0.000       0.003       0.007\n",
      "==============================================================================\n",
      "Omnibus:                    16037.860   Durbin-Watson:                   1.821\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1628790.543\n",
      "Skew:                           2.127   Prob(JB):                         0.00\n",
      "Kurtosis:                      41.759   Cond. No.                     9.46e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.46e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "class LinearReg_sm(model):\n",
    "    \n",
    "    def fit_model(self, X_train, y_train):\n",
    "\n",
    "        predictors = [\n",
    "            'mv',\n",
    "            'bvtmv',\n",
    "            'ret_12m',\n",
    "            'retxs_12m',\n",
    "            'ret_6m',\n",
    "            'std_12m',\n",
    "            'fcf',\n",
    "            'roic',\n",
    "            'dy',\n",
    "            'beta',\n",
    "            'mom_3m12m',\n",
    "            'mom_12m36m'\n",
    "        ]\n",
    "        self.predictors = predictors\n",
    "        self.X_train = X_train[predictors]\n",
    "        self.y_train = y_train\n",
    "\n",
    "        model = sm.OLS(y_train, X_train[predictors]).fit()\n",
    "        self.model = model\n",
    "        self.summary = model.summary()\n",
    "    \n",
    "    def test_model(self, X_test, y_test):\n",
    "        self.X_test = X_test[self.predictors]\n",
    "        self.y_test = y_test\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        self.r2_oos  = r2_score(y_test, pred)\n",
    "        self.r2_is   = r2_score(self.y_train, self.model.predict(self.X_train))\n",
    "\n",
    "LR= LinearReg_sm()\n",
    "LR.fit_model(X_train, y_train)\n",
    "LR.test_model(X_test, y_test)\n",
    "\n",
    "print(\"Linear       \", np.mean(cross_val_score(estimator=LinearRegression(), X=X[LR.predictors], y=y, scoring='r2', cv=10)))\n",
    "print(LR.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Polynomial Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial    -0.07638820538320282\n"
     ]
    }
   ],
   "source": [
    "class PolynomialReg_sm(model):\n",
    "\n",
    "    def fit_model(self, X_train, y_train):\n",
    "        \n",
    "        # Same as linear regression but with non-linear predictor set (eg. X^2 terms)\n",
    "        predictors = [\n",
    "            'bvtmv','bvtmv_^2','bvtmv_^3', \n",
    "            'retxs_12m','retxs_12m_^2','retxs_12m_^3', \n",
    "            'std_12m','std_12m_^2','std_12m_^3',\n",
    "            'fcf','fcf_^2','fcf_^3', \n",
    "            'mv','mv_^0.5', \n",
    "            'roic','roic_^2','roic_^3', \n",
    "            'dy','dy_^2','dy_^3', \n",
    "            'beta','beta_^2','beta_^3', \n",
    "            'so', 'so_^2'\n",
    "        ]\n",
    "        \n",
    "        self.X_train = X_train[predictors]\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.predictors = predictors\n",
    "\n",
    "        model = sm.OLS(y_train, self.X_train).fit()\n",
    "        self.model = model\n",
    "        self.summary = self.model.summary()\n",
    "\n",
    "    def test_model(self, X_test, y_test):\n",
    "        self.X_test = X_test[self.predictors]\n",
    "        self.y_test = y_test\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        self.r2_oos  = r2_score(self.y_test, pred)\n",
    "        self.r2_is   = r2_score(self.y_train, self.model.predict(self.X_train))\n",
    "\n",
    "PR = PolynomialReg_sm()\n",
    "PR.fit_model(X_train, y_train)\n",
    "PR.test_model(X_test, y_test)\n",
    "\n",
    "print(\"Polynomial   \", np.mean(cross_val_score(estimator=LinearRegression(), X=X[PR.predictors], y=y, scoring='r2', cv=10)))\n",
    "\n",
    "# print(PR.r2_oos)\n",
    "# print(PR.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Regression Tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegTree(model):\n",
    "    \n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "\n",
    "    def fit_model(self, X_train, y_train):\n",
    "        \n",
    "        # Same as linear regression but with non-linear predictor set (eg. X^2 terms)\n",
    "        predictors = PR.predictors\n",
    "        \n",
    "        self.X_train = X_train[predictors]\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.predictors = predictors\n",
    "\n",
    "        self.fitted_model = self.model.fit(self.X_train, self.y_train)\n",
    "        self.summary = \"Regression tree has no summary\"\n",
    "\n",
    "    def test_model(self, X_test, y_test):\n",
    "        self.X_test = X_test[self.predictors]\n",
    "        self.y_test = y_test\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        self.r2_oos  = r2_score(y_test, pred)\n",
    "        self.r2_is   = r2_score(self.y_train, self.model.predict(self.X_train))\n",
    "\n",
    "\n",
    "RT = RegTree(max_depth=2)\n",
    "# RT.fit_model(X_train, y_train)\n",
    "# RT.test_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Neural Networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 25712 samples\n",
      "Epoch 1/5\n",
      "25712/25712 - 8s - loss: 1.0013\n",
      "Epoch 2/5\n",
      "25712/25712 - 8s - loss: 1.0013\n",
      "Epoch 3/5\n",
      "25712/25712 - 8s - loss: 1.0013\n",
      "Epoch 4/5\n",
      "25712/25712 - 8s - loss: 1.0013\n",
      "Epoch 5/5\n",
      "25712/25712 - 8s - loss: 1.0013\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def norm(x):\n",
    "    return (x-x.describe().transpose()['mean'])/x.describe().transpose()['std']\n",
    "\n",
    "normXtrain = norm(X_train.drop('const', axis=1))\n",
    "normXtest = norm(X_test.drop('const', axis=1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim=len(normXtrain.columns), activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(normXtrain, y_train, epochs=5, batch_size=2, verbose=2)\n",
    "\n",
    "model.predict(normXtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear        -0.01653885571103586\n",
      "Tree          0.0014704875934970385\n",
      "Polynomial    -0.07638820538320282\n",
      "Lasso         -0.009879764986742434\n",
      "Ridge         -0.07638031526471364\n"
     ]
    }
   ],
   "source": [
    "cv = 10\n",
    "\n",
    "print(\"Linear       \", np.mean(cross_val_score(estimator=LinearRegression(), X=X[LR.predictors], y=y, scoring='r2', cv=cv)))\n",
    "print(\"Tree         \", np.mean(cross_val_score(estimator=DecisionTreeRegressor(max_depth=2), X=X[PR.predictors], y=y, scoring='r2', cv=cv)))\n",
    "print(\"Polynomial   \", np.mean(cross_val_score(estimator=LinearRegression(), X=X[PR.predictors], y=y, scoring='r2', cv=cv)))\n",
    "print(\"Lasso        \", np.mean(cross_val_score(estimator=Lasso(alpha=1.5), X=X[PR.predictors], y=y, scoring='r2', cv=cv)))\n",
    "print(\"Ridge        \", np.mean(cross_val_score(estimator=Ridge(alpha=1.5), X=X[PR.predictors], y=y, scoring='r2', cv=cv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def describe_r2(model, predictors):\n",
    "    r2_ts = pd.DataFrame(columns=['model_R2'], index=dates)\n",
    "\n",
    "    for i in range(1,len(dates)-37):\n",
    "        X_train, y_train, X_test, y_test = prepare_data(i, 36)\n",
    "        X_train, X_test = X_train[predictors], X_test[predictors]\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            r2_ts['model_R2'].iloc[i] = r2_score(y_test, pred)\n",
    "\n",
    "        except ValueError as err:\n",
    "            print(err)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim(-5, 1)\n",
    "    ax.plot(r2_ts)\n",
    "    plt.legend(r2_ts.columns)\n",
    "    print(\"Mean:  \", r2_ts.mean())\n",
    "\n",
    "describe_r2(model=DecisionTreeRegressor(max_depth=2), predictors=LR.predictors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
